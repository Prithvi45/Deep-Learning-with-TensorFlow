{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Convolution ?\n",
    "\n",
    "* g(x,y) = h(x,y)*f(x,y)\n",
    "\n",
    "* Convolution can achieve something,  that the previous two methods of manipulating images can’t achieve. Those include the blurring, sharpening, edge detection, noise reduction e.t.c.\n",
    "\n",
    "### Applications in Image Processing\n",
    "* Blurring, line and edge detections\n",
    "* Sobel method, gaussian, laplacian masks are available to achieve this\n",
    "\n",
    "## Formulae\n",
    "* Definition of 1D Convolution : y[n]=x[n]*h[n] = summ(x[n].h[n-k])    k-> -inifinite to infinite\n",
    "\n",
    "\n",
    "#### Learn all deep learning layer apis of tensorflow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([6,2])\n",
    "h = np.array([1,2,5,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.convolve(x,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([5,6,7])\n",
    "c = np.convolve(a,b)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply convolution on  1D array and 2D image  using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n",
    "* Convolution ->  relu(activation) --> pooling -> relu(activation)  -> Dropouts -> Probability conversion(softmax)\n",
    "* Relu Activation function : used to handle non linear data\n",
    "* Relu is used for Normalization\n",
    "* Relu fires 0 for all negative values\n",
    "* pooling-> picks max value from applied patch on array\n",
    "* backpropogation -> Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_batch = tf.constant([\n",
    "        [  # First Image\n",
    "            [[0, 255, 0], [0, 255, 0], [0, 255, 0]],\n",
    "            [[0, 255, 0], [0, 255, 0], [0, 255, 0]]\n",
    "        ],\n",
    "        [  # Second Image\n",
    "            [[0, 0, 255], [0, 0, 255], [0, 0, 255]],\n",
    "            [[0, 0, 255], [0, 0, 255], [0, 0, 255]]\n",
    "        ]\n",
    "    ])\n",
    "image_batch.get_shape()\n",
    "\n",
    "#below dimensions are as -> rows of image 1, rows of image 2 , columns, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_batch = tf.constant([\n",
    "        [  # First Input\n",
    "            [[0.0], [1.0]],\n",
    "            [[2.0], [3.0]]\n",
    "        ],\n",
    "        [  # Second Input\n",
    "            [[2.0], [4.0]],\n",
    "            [[6.0], [8.0]]\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "kernel = tf.constant([\n",
    "        [\n",
    "            [[1.0, 2.0]]\n",
    "        ]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2d = tf.nn.conv2d(input_batch, kernel, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_right_image_pixel = sess.run(input_batch)[0][1][1]\n",
    "lower_right_kernel_pixel = sess.run(conv2d)[0][1][1]\n",
    "lower_right_image_pixel, lower_right_kernel_pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strides\n",
    "The value of convolutions in computer vision is their ability to reduce the dimensionality of the input, which is an image in this case. An image’s dimensionality (2D image) is its width, height and number of channels. A large image dimensionality requires an exponentially larger amount of time for a neural network to scan over every pixel and judge which ones are important. Reducing dimensionality of an image with convolutions is done by altering the strides of the kernel.\n",
    "\n",
    "The parameter strides, causes a kernel to skip over pixels of an image and not include them in the output. It’s not fair to say the pixels are skipped because they still may affect the output. The strides parameter highlights how a convolution operation is working with a kernel when a larger image and more complex kernel are used. As a convolution is sliding the kernel over the input, it’s using the strides parameter to change how it walks over the input. Instead of going over every element of an input, the strides parameter could configure the convolution to skip certain elements.\n",
    "\n",
    "For example, take the convolution of a larger image and a larger kernel. In this case, it’s a convolution between a 6 pixel tall, 6 pixel wide and 1 channel deep image (6x6x1) and a (3x3x1) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_batch = tf.constant([\n",
    "        [  # First Input (6x6x1)\n",
    "            [[0.0], [1.0], [2.0], [3.0], [4.0], [5.0]],\n",
    "            [[0.1], [1.1], [2.1], [3.1], [4.1], [5.1]],\n",
    "            [[0.2], [1.2], [2.2], [3.2], [4.2], [5.2]],\n",
    "            [[0.3], [1.3], [2.3], [3.3], [4.3], [5.3]],\n",
    "            [[0.4], [1.4], [2.4], [3.4], [4.4], [5.4]],\n",
    "            [[0.5], [1.5], [2.5], [3.5], [4.5], [5.5]],\n",
    "        ],\n",
    "    ])\n",
    "\n",
    "kernel = tf.constant([  # Kernel (3x3x1)\n",
    "        [[[0.0]], [[0.5]], [[0.0]]],\n",
    "        [[[0.0]], [[1.0]], [[0.0]]],\n",
    "        [[[0.0]], [[0.5]], [[0.0]]]\n",
    "    ])\n",
    "\n",
    "# NOTE: the change in the size of the strides parameter.\n",
    "conv2d = tf.nn.conv2d(input_batch, kernel, strides=[1, 3, 3, 1], padding='SAME')\n",
    "sess.run(conv2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "When a kernel is overlapped on an image it should be set to fit within the bounds of the image. At times, the sizing may not fit and a good alternative is to fill the missing area in the image. Filling the missing area of the image is known as padding the image. TensorFlow will pad the image with zeros or raise an error when the sizes don’t allow a kernel to stride over an image without going past its bounds. The amount of zeros or the error state of tf.nn.conv2d is controlled by the parameter padding which has two possible values ('VALID', ‘SAME').\n",
    "\n",
    "SAME: The convolution output is the SAME size as the input. This doesn’t take the filter’s size into account when calculating how to stride over the image. This may stride over more of the image than what exists in the bounds while padding all the missing values with zero.\n",
    "\n",
    "VALID: Take the filter’s size into account when calculating how to stride over the image. This will try to keep as much of the kernel inside the image’s bounds as possible. There may be padding in some cases but will avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Detection example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = tf.constant([\n",
    "        [\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]],\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]],\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]]\n",
    "        ],\n",
    "        [\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]],\n",
    "            [[ 8., 0., 0.], [ 0., 8., 0.], [ 0., 0., 8.]],\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]]\n",
    "        ],\n",
    "        [\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]],\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]],\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]]\n",
    "        ]\n",
    "    ], )\n",
    "\n",
    "image_batch = tf.cast(image_batch, tf.float32)\n",
    "conv2d = tf.nn.conv2d(image_batch, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "activation_map = sess.run(tf.minimum(tf.nn.relu(conv2d), 255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output created from convolving an image with an edge detection kernel are all the areas where and edge was detected. The code assumes a batch of images is already available (image_batch) with a real image loaded from disk. In this case, the image is an example image found in the Stanford Dogs Dataset. The kernel has three input and three output channels. The channels sync up to RGB values between left-bracket 0 comma 255 right-bracket with 255 being the maximum intensity. The tf.minimum and tf.nn.relu calls are there to keep the convolution values within the range of valid RGB colors of left-bracket 0 comma 255 right-bracket.\n",
    "\n",
    "\n",
    "There are many other) common kernels which can be used in this simplified example. Each will highlight different patterns in an image with different results. The following kernel will sharpen an image by increasing the intensity of color changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = tf.constant([\n",
    "        [\n",
    "            [[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]],\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]],\n",
    "            [[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]\n",
    "        ],\n",
    "        [\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]],\n",
    "            [[ 5., 0., 0.], [ 0., 5., 0.], [ 0., 0., 5.]],\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]]\n",
    "        ],\n",
    "        [\n",
    "            [[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]],\n",
    "            [[ -1., 0., 0.], [ 0., -1., 0.], [ 0., 0., -1.]],\n",
    "            [[ 0, 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "image_batch = tf.cast(image_batch, tf.float32)\n",
    "conv2d = tf.nn.conv2d(image_batch, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "activation_map = sess.run(tf.minimum(tf.nn.relu(conv2d), 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow()\n",
    "activation_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Layers\n",
    "For a neural network architecture to be considered a CNN, it requires at least one convolution layer (tf.nn.conv2d). There are practical uses for a single layer CNN (edge detection), for image recognition and categorization it is common to use different layer types to support a convolution layer. These layers help reduce over-fitting, speed up training and decrease memory usage.\n",
    "\n",
    "The layers covered in this chapter are focused on layers commonly used in a CNN architecture. A CNN isn’t limited to use only these layers, they can be mixed with layers designed for other network architectures.\n",
    "\n",
    "\n",
    "\n",
    "### Convolution Layers\n",
    "One type of convolution layer has been covered in detail (tf.nn.conv2d) but there are a few notes which are useful to advanced users. The convolution layers in TensorFlow don’t do a full convolution, details can be found in the TensorFlow API documentation. In practice, the difference between a convolution and the operation TensorFlow uses is performance. TensorFlow uses a technique to speed up the convolution operation in all the different types of convolution layers.\n",
    "There are use cases for each type of convolution layer but for tf.nn.conv2d is a good place to start. The other types of convolutions are useful but not required in building a network capable of object recognition and classification. A brief summary of each is included.\n",
    "\n",
    "TF.NN.DEPTHWISE_CONV2D\n",
    "This convolution is used when attaching the output of one convolution to the input of another convolution layer. An advanced use case is using a tf.nn.depthwise_conv2d to create a network following the inception architecture.\n",
    "\n",
    "TF.NN.SEPARABLE_CONV2D\n",
    "This is similar to tf.nn.conv2d, but not a replacement for it. For large models, it speeds up training without sacrificing accuracy. For small models, it will converge quickly with worse accuracy.\n",
    "\n",
    "TF.NN.CONV2D_TRANSPOSE\n",
    "This applies a kernel to a new feature map where each section is filled with the same values as the kernel. As the kernel strides over the new image, any overlapping sections are summed together. There is a great explanation on how tf.nn.conv2d_transpose is used for learnable upsampling in Stanford’s CS231n Winter 2016: Lecture 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Activation Functions\n",
    "These functions are used in combination with the output of other layers to generate a feature map. They’re used to smooth (or differentiate) the results of certain operations. The goal is to introduce non-linearity into the neural network. Non-linearity means that the input is a curve instead of a straight line. Curves are capable of representing more complex changes in input. For example, non-linear input is capable of describing input which stays small for the majority of the time but periodically has a single point at an extreme. Introduction of non-linearity in a neural network allows it to train on the complex patterns found in data.\n",
    "\n",
    "TensorFlow has multiple activation functions available. With CNNs, tf.nn.relu is primarily used because of its performance although it sacrifices information. When starting out, using tf.nn.relu is recommended but advanced users may create their own. When considering if an activation function is useful there are a few primary considerations.\n",
    "\n",
    "1. The function is monotonic, so its output should always be increasing or decreasing along with the input. This allows gradient descent optimization to search for local minima.\n",
    "\n",
    "2. The function is differentiable, so there must be a derivative at any point in the function’s domain. This allows gradient descent optimization to properly work using the output from this style of activation function.\n",
    "\n",
    "Any functions that satisfy those considerations could be used as activation functions. In TensorFlow there are a few worth highlighting which are common to see in CNN architectures. A brief summary of each is included with a small sample code illustrating their usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.NN.RELU\n",
    "A rectifier (rectified linear unit) called a ramp function in some documentation and looks like a skateboard ramp when plotted. ReLU is linear and keeps the same input values for any positive numbers while setting all negative numbers to be 0. It has the benefits that it doesn’t suffer from gradient vanishing and has a range of left-bracket 0 comma plus normal infinity right-parenthesis. A drawback of ReLU is that it can suffer from neurons becoming saturated when too high of a learning rate is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = tf.range(-2, 3)\n",
    "relu_out = tf.nn.relu(features)\n",
    "# Keep note of the value for negative features\n",
    "sess.run([features, relu_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.SIGMOID\n",
    "A sigmoid function returns a value in the range of left-bracket 0.0 comma 1.0 right-bracket. Larger values sent into a tf.sigmoid will trend closer to 1.0 while smaller values will trend towards 0.0. The ability for sigmoids to keep a values between left-bracket 0.0 comma 1.0 right-bracket is useful in networks which train on probabilities which are in the range of  left-bracket 0.0 comma 1.0 right-bracket. The reduced range of output values can cause trouble with input becoming saturated and changes in input becoming exaggerated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note, tf.sigmoid (tf.nn.sigmoid) is currently limited to float values\n",
    "features = tf.to_float(tf.range(-1, 3))\n",
    "sess.run([features, tf.sigmoid(features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.TANH\n",
    "\n",
    "A hyperbolic tangent function (tanh) is a close relative to tf.sigmoid with some of the same benefits and drawbacks. The main difference between tf.sigmoid and tf.tanh is that tf.tanh has a range of left-bracket negative 1.0 comma 1.0 right-bracket. The ability to output negative values may be useful in certain network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note, tf.tanh (tf.nn.tanh) is currently limited to float values\n",
    "features = tf.to_float(tf.range(-1, 3))\n",
    "sess.run([features, tf.tanh(features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.NN.DROPOUT\n",
    "Set the output to be 0.0 based on a configurable probability. This layer performs well in scenarios where a little randomness helps training. An example scenario is when there are patterns being learned that are too tied to their neighboring features. This layer will add a little noise to the output being learned.\n",
    "\n",
    "NOTE: This layer should only be used during training because the random noise it adds will give misleading results while testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = tf.constant([-0.1, 0.0, 0.1, 0.2])\n",
    "# Note, the output should be different on almost ever execution. Your numbers won't match\n",
    "# this output.\n",
    "sess.run([features, tf.nn.dropout(features, keep_prob=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the output has a 50% probability of being kept. Each execution of this layer will have different output (most likely, it’s somewhat random). When an output is dropped, its value is set to 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "Pooling layers reduce over-fitting and improving performance by reducing the size of the input. They’re used to scale down input while keeping important information for the next layer. It’s possible to reduce the size of the input using a tf.nn.conv2d alone but these layers execute much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.NN.MAX_POOL\n",
    "Strides over a tensor and chooses the maximum value found within a certain kernel size. Useful when the intensity of the input data is relevant to importance in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Usually the input would be output from a previous layer and not an image directly.\n",
    "# Our task is to find largest valus from the array\n",
    "batch_size=1\n",
    "input_height = 3\n",
    "input_width = 3\n",
    "input_channels = 1\n",
    "\n",
    "layer_input = tf.constant([\n",
    "        [\n",
    "            [[1.0], [0.2], [1.5]],\n",
    "            [[0.1], [1.2], [1.4]],\n",
    "            [[1.1], [0.4], [0.4]]\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "# The strides will look at the entire input by using the image_height and image_width\n",
    "kernel = [batch_size, input_height, input_width, input_channels]\n",
    "max_pool = tf.nn.max_pool(layer_input, kernel, [1, 1, 1, 1], \"VALID\")\n",
    "sess.run(max_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer_input is a tensor with a shape similar to the output of tf.nn.conv2d or an activation function. The goal is to keep only one value, the largest value in the tensor. In this case, the largest value of the tensor is 1.5 and is returned in the same format as the input. If the kernel were set to be smaller, it would choose the largest value in each kernel size as it strides over the image.\n",
    "\n",
    "Max-pooling will commonly be done using 2x2 receptive field (kernel with a height of 2 and width of 2) which is often written as a “2x2 max-pooling operation”. One reason to use a 2x2 receptive field is that it’s the smallest amount of downsampling which can be done in a single pass. If a 1x1 receptive field were used then the output would be the same as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.NN.AVG_POOL\n",
    "Strides over a tensor and averages all the values at each depth found within a kernel size. Useful when reducing values where the entire kernel is important, for example, input tensors with a large width and height but small depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "input_height = 3\n",
    "input_width = 3\n",
    "input_channels = 1\n",
    "\n",
    "layer_input = tf.constant([\n",
    "        [\n",
    "            [[1.0], [1.0], [1.0]],\n",
    "            [[1.0], [0.5], [0.0]],\n",
    "            [[0.0], [0.0], [0.0]]\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "# The strides will look at the entire input by using the image_height and image_width\n",
    "kernel = [batch_size, input_height, input_width, input_channels]\n",
    "max_pool = tf.nn.avg_pool(layer_input, kernel, [1, 1, 1, 1], \"VALID\")\n",
    "sess.run(max_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "Normalization layers are not unique to CNNs and aren’t used as often. When using tf.nn.relu, it is useful to consider normalization of the output. Since ReLU is unbounded, it’s often useful to utilize some form of normalization to identify high-frequency features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.NN.LOCAL_RESPONSE_NORMALIZATION (TF.NN.LRN)\n",
    "Local response normalization is a function which shapes the output based on a summation operation best explained in TensorFlow’s documentation.\n",
    "\n",
    "... Within a given vector, each component is divided by the weighted, squared sum of inputs within depth_radius.\n",
    "\n",
    "One goal of normalization is to keep the input in a range of acceptable numbers. For instance, normalizing input in the range of left-bracket 0.0 comma 1.0 right-bracket where the full range of possible values is normalized to be represented by a number greater than or equal to 0.0 and less than or equal to 1.0. Local response normalization normalizes values while taking into account the significance of each value.\n",
    "\n",
    "Cuda-Convnet includes further details on why using local response normalization is useful in some CNN architectures. ImageNet uses this layer to normalize the output from tf.nn.relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a range of 3 floats.\n",
    "# [0,1]\n",
    "#  TensorShape([batch, image_height, image_width, image_channels])\n",
    "layer_input = tf.constant([\n",
    "        [[[ 1.]], [[ 2.]], [[ 3.]]]\n",
    "    ])\n",
    "\n",
    "lrn = tf.nn.local_response_normalization(layer_input)\n",
    "sess.run([layer_input, lrn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Layers\n",
    "TensorFlow has introduced high level layers designed to make it easier to create fairly standard layer definitions. These aren’t required to use but they help avoid duplicate code while following best practices. While getting started, these layers add a number of non-essential nodes to the graph. It’s worth waiting until the basics are comfortable before using these layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.CONTRIB.LAYERS.CONVOLUTION2D\n",
    "The convolution2d layer will do the same logic as tf.nn.conv2d while including weight initialization, bias initialization, trainable variable output, bias addition and adding an activation function. Many of these steps haven’t been covered for CNNs yet but should be familiar. A kernel is a trainable variable (the CNN’s goal is to train this variable), weight initialization is used to fill the kernel with values (tf.truncated_normal) on its first run. The rest of the parameters are similar to what have been used before except they are reduced to short-hand version. Instead of declaring the full kernel, now it’s a simple tuple (1,1) for the kernel’s height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_input = tf.constant([\n",
    "            [\n",
    "                [[0., 0., 0.], [255., 255., 255.], [254., 0., 0.]],\n",
    "                [[0., 191., 0.], [3., 108., 233.], [0., 191., 0.]],\n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.]]\n",
    "            ]\n",
    "        ])\n",
    "\n",
    "conv2d = tf.contrib.layers.convolution2d(\n",
    "    image_input,\n",
    "#     num_output_channels=4,     this is deprecated , use below one\n",
    "    num_outputs=4,\n",
    "    kernel_size=(1,1),          # It's only the filter height and width.\n",
    "    activation_fn=tf.nn.relu,\n",
    "    stride=(1, 1),              # Skips the stride values for image_batch and input_channels.\n",
    "    trainable=True)\n",
    "\n",
    "# It's required to initialize the variables used in convolution2d's setup.\n",
    "# sess.run(tf.initialize_all_variables())   derecated , use below function\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(conv2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: tf.to_float should not be used if the input is an image, instead use tf.image.convert_image_dtype which will properly change the range of values used to describe colors. In this example code, float values of 255. were used which aren’t what TensorFlow expects when is sees an image using float values. TensorFlow expects an image with colors described as floats to stay in the range of left-bracket 0 comma 1 right-bracket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TF.CONTRIB.LAYERS.FULLY_CONNECTED\n",
    "A fully connected layer is one where every input is connected to every output. This is a fairly common layer in many architectures but for CNNs, the last layer is quite often fully connected. The tf.contrib.layers.fully_connected layer offers a great short-hand to create this last layer while following best practices.\n",
    "\n",
    "Typical fully connected layers in TensorFlow are often in the format of tf.matmul(features, weight) + bias where feature, weight and bias are all tensors. This short-hand layer will do the same thing while taking care of the intricacies involved in managing the weight and bias tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = tf.constant([\n",
    "        [[1.2], [3.4]]\n",
    "    ])\n",
    "\n",
    "fc = tf.contrib.layers.fully_connected(features, num_outputs=2)\n",
    "\n",
    "# It's required to initialize all the variables first or there'll be an error about precondition failures.\n",
    "# sess.run(tf.initialize_all_variables())\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images and TensorFlow\n",
    "TensorFlow is designed to support working with images as input to neural networks. TensorFlow supports loading common file formats (JPG, PNG), working in different color spaces (RGB, RGBA) and common image manipulation tasks. TensorFlow makes it easier to work with images but it’s still a challenge. The largest challenge working with images are the size of the tensor which is eventually loaded. Every image requires a tensor the same size as the image’s h e i g h t asterisk w i d t h asterisk c h a n n e l s (h.w.c) . As a reminder, channels are represented as a rank 1 tensor including a scalar amount of color in each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A red RGB pixel in TensorFlow would be represented with the following tensor.\n",
    "red = tf.constant([255, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The match_filenames_once will accept a regex but there is no need for this example.\n",
    "# image_filename = \"prithvi.png\"\n",
    "# filename_queue = tf.train.string_input_producer(\n",
    "#     tf.train.match_filenames_once(image_filename))\n",
    "filename_queue = tf.train.string_input_producer(['prithvi.jpg']) \n",
    "image_reader = tf.WholeFileReader()\n",
    "_, image_file = image_reader.read(filename_queue)\n",
    "image = tf.image.decode_jpeg(image_file)\n",
    "\n",
    "\n",
    "\n",
    "##this method is not working , needs a debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess.run(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 35, 3)\n"
     ]
    }
   ],
   "source": [
    "## Reading multiple images\n",
    "##Another method\n",
    "\n",
    "from PIL import Image\n",
    "filename_queue = tf.train.string_input_producer([r'C:\\Users\\omc\\Desktop\\Deep Learning with Tensorflow\\prithvi.jpg']) #  list of files to read\n",
    "reader = tf.WholeFileReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "my_img = tf.image.decode_jpeg(value) # use png or jpg decoder based on your files.\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init_op)\n",
    "\n",
    "  # Start populating the filename queue.\n",
    "\n",
    "  coord = tf.train.Coordinator()\n",
    "  threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "  for i in range(1): #length of your filename list\n",
    "    image = my_img.eval() #here is your image Tensor :)\n",
    "    #above command is similar to this tf.get_default_session().run(t) \n",
    "\n",
    "  print(image.shape)\n",
    "  Image.fromarray(np.asarray(image)).show()    #displays image\n",
    "\n",
    "  coord.request_stop()\n",
    "  coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQSeEkJjABZ0QQmLC/wOaqOymqdgQ9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbb9af98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display image using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(image)\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as g:\n",
    "#     g.run(my_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image\n",
    "#notice that it’s a fairly simple rank 3 tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow supports two image file formats Jpeg and Png for decoding\n",
    "* Jpeg does not have alpha paramenter\n",
    "* Png has aplha\n",
    "* Aplha -> Transparency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRECORD\n",
    "TensorFlow has a built-in file format designed to keep binary data and label (category for training) data in the same file. The format is called TFRecord and the format requires a preprocessing step to convert images to a TFRecord format before training. The largest benefit is keeping each input image in the same file as the label associated with it.\n",
    "\n",
    "Technically, TFRecord files are protobuf formatted files. They are great for use as a preprocessed format because they aren’t compressed and can be loaded into memory quickly. In this example, an image is written to a new TFRecord formatted file and it’s label is stored as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reuse the image from earlier and give it a fake label\n",
    "image_label = b'\\x01'  # Assume the label data is in a one-hot representation (00000001)\n",
    "\n",
    "# Convert the tensor into bytes, notice that this will load the entire image file\n",
    "# image_loaded = sess.run(image)\n",
    "image_loaded = image\n",
    "image_bytes = image_loaded.tobytes()\n",
    "image_height, image_width, image_channels = image_loaded.shape\n",
    "\n",
    "# Export TFRecord\n",
    "writer = tf.python_io.TFRecordWriter(\"./output/training-image.tfrecord\")\n",
    "\n",
    "# Don't store the width, height or image channels in this Example file to save space but not required.\n",
    "example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'label': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_label])),\n",
    "            'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))\n",
    "        }))\n",
    "\n",
    "# This will save the example to a text file tfrecord\n",
    "writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label is in a format known as one-hot encoding which is a common way to work with label data for categorization of multi-class data. The Stanford Dogs Dataset is being treated as multi-class data because the dogs are being categorized as a single breed and not a mix of breeds. In the real world, a multilabel solution would work well to predict dog breeds because it’d be capable of matching a dog with multiple breeds.\n",
    "\n",
    "In the example code, the image is loaded into memory and converted into an array of bytes. The bytes are then added to the tf.train.Example file which are serialized SerializeToString before storing to disk. Serialization is a way of converting the in memory object into a format safe to be transferred to a file. The serialized example is now saved in a format which can be loaded and deserialized back to the example format saved here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading back our tfrecord image\n",
    "Now that the image is saved as a TFRecord it can be loaded again but this time from the TFRecord file. This would be the loading required in a training step to load the image and label for training. This will save time from loading the input image and its corresponding label separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load TFRecord\n",
    "tf_record_filename_queue = tf.train.string_input_producer(\n",
    "    tf.train.match_filenames_once(\"./output/training-image.tfrecord\"))\n",
    "\n",
    "# Notice the different record reader, this one is designed to work with TFRecord files which may\n",
    "# have more than one example in them.\n",
    "tf_record_reader = tf.TFRecordReader()\n",
    "_, tf_record_serialized = tf_record_reader.read(tf_record_filename_queue)\n",
    "\n",
    "# The label and image are stored as bytes but could be stored as int64 or float64 values in a\n",
    "# serialized tf.Example protobuf.\n",
    "tf_record_features = tf.parse_single_example(\n",
    "    tf_record_serialized,\n",
    "    features={\n",
    "        'label': tf.FixedLenFeature([], tf.string),\n",
    "        'image': tf.FixedLenFeature([], tf.string),\n",
    "    })\n",
    "\n",
    "# Using tf.uint8 because all of the channel information is between 0-255\n",
    "tf_record_image = tf.decode_raw(\n",
    "    tf_record_features['image'], tf.uint8)\n",
    "\n",
    "# Reshape the image to look like the image saved, not required\n",
    "tf_record_image = tf.reshape(\n",
    "    tf_record_image,\n",
    "    [image_height, image_width, image_channels])\n",
    "# Use real values for the height, width and channels of the image because it's required\n",
    "# to reshape the input.\n",
    "\n",
    "tf_record_label = tf.cast(tf_record_features['label'], tf.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, the file is loaded in the same way as any other file. The main difference is that the file is then read using a TFRecordReader. Instead of decoding the image, the TFRecord is parsed tf.parse_single_example and then the image is read as raw bytes (tf.decode_raw).\n",
    "\n",
    "After the file is loaded, it is reshaped (tf.reshape) in order to keep it in the same layout as tf.nn.conv2d expects it [image_height, image_width, image_channels]. It’d be save to expand the dimensions (tf.expand) in order to add in the batch_size dimension to the input_batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following code is useful to check that the image saved to disk is the same as the image which was loaded from TensorFlow.\n",
    "# sess.run(tf.equal(image, tf_record_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### output:\n",
    "    array([[[ True, True, True],\n",
    "    [ True, True, True],\n",
    "    [ True, True, True]],\n",
    "    [[ True, True, True],\n",
    "    [ True, True, True],\n",
    "    [ True, True, True]],\n",
    "    [[ True, True, True],\n",
    "    [ True, True, True],\n",
    "    [ True, True, True]]], dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Check that the label is still 0b00000001.\n",
    "# sess.run(tf_record_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Image Manipulation\n",
    "CNNs work well when they’re given a large amount of diverse quality training data. Images capture complex scenes in a way which visually communicates an intended subject. In the Stanford Dog’s Dataset, it’s important that the images visually highlight the importance of dogs in the picture. A picture with a dog clearly visible in the center is considered more valuable than one with a dog in the background.\n",
    "\n",
    "TensorFlow is not designed as an image manipulation framework. There are libraries available in Python which support more image manipulation than TensorFlow (PIL and OpenCV). For TensorFlow, we’ll summarize a few useful image manipulation features available which are useful in training CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROPPING\n",
    "Cropping an image will remove certain regions of the image without keeping any information. Cropping is similar to tf.slice where a section of a tensor is cut out from the full tensor. Cropping an input image for a CNN can be useful if there is extra input along a dimension which isn’t required. For example, cropping dog pictures where the dog is in the center of the images to reduce the size of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[140, 114, 113]]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_image =  tf.image.central_crop(image, 0.1)\n",
    "sess.run(cropped_image)\n",
    "#this will crop 10% of input image , this will operate by center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[208, 217, 232],\n",
       "        [ 18,  25,  41],\n",
       "        [  0,   0,  12],\n",
       "        ..., \n",
       "        [  0,   2,  16],\n",
       "        [ 99, 108, 123],\n",
       "        [223, 235, 251]],\n",
       "\n",
       "       [[222, 231, 246],\n",
       "        [ 62,  68,  82],\n",
       "        [  1,   3,  15],\n",
       "        ..., \n",
       "        [  0,   0,  12],\n",
       "        [116, 123, 139],\n",
       "        [217, 229, 245]],\n",
       "\n",
       "       [[234, 242, 255],\n",
       "        [102, 106, 118],\n",
       "        [  3,   1,  12],\n",
       "        ..., \n",
       "        [  6,   8,  21],\n",
       "        [150, 156, 170],\n",
       "        [216, 225, 242]],\n",
       "\n",
       "       ..., \n",
       "       [[228, 248, 255],\n",
       "        [219, 242, 250],\n",
       "        [214, 234, 243],\n",
       "        ..., \n",
       "        [202, 221, 228],\n",
       "        [202, 222, 233],\n",
       "        [203, 225, 238]],\n",
       "\n",
       "       [[219, 241, 252],\n",
       "        [205, 229, 239],\n",
       "        [208, 231, 239],\n",
       "        ..., \n",
       "        [192, 210, 220],\n",
       "        [191, 213, 226],\n",
       "        [194, 217, 231]],\n",
       "\n",
       "       [[219, 241, 254],\n",
       "        [199, 223, 233],\n",
       "        [207, 229, 240],\n",
       "        ..., \n",
       "        [187, 207, 218],\n",
       "        [186, 208, 222],\n",
       "        [187, 210, 226]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_image =  tf.image.central_crop(image, 0.5)\n",
    "sess.run(cropped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# fig.add_subplot(1,2,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[222, 232, 244]],\n",
       "\n",
       "       [[224, 234, 246]]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This crop method only works on real value input.\n",
    "real_image = image\n",
    "\n",
    "bounding_crop = tf.image.crop_to_bounding_box(\n",
    "    real_image, offset_height=0, offset_width=0, target_height=2, target_width=1)\n",
    "\n",
    "sess.run(bounding_crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PADDING\n",
    "Pad an image with zeros in order to make it the same size as an expected image. This can be accomplished using tf.pad but TensorFlow has another function useful for resizing images which are too large or too small. The method will pad an image which is too small including zeros along the edges of the image. Often, this method is used to resize small images because any other method of resizing with distort the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[222, 232, 244],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[224, 234, 246],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]]], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This padding method only works on real value input.\n",
    "# real_image = sess.run(image)\n",
    "real_image = bounding_crop\n",
    "pad = tf.image.pad_to_bounding_box(\n",
    "    real_image, offset_height=0, offset_width=0, target_height=2, target_width=3)\n",
    "\n",
    "sess.run(pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example code increases the images height by one pixel and its width by a pixel as well. The new pixels are all set to 0. Padding in this manner is useful for scaling up an image which is too small. This can happen if there are images in the training set with a mix of aspect ratios. TensorFlow has a useful shortcut for resizing images which don’t match the same aspect ratio using a combination of pad and crop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[159, 128, 143],\n",
       "        [151, 121, 121],\n",
       "        [147, 121, 120],\n",
       "        [138, 116, 118],\n",
       "        [173, 157, 160]],\n",
       "\n",
       "       [[162, 134, 148],\n",
       "        [130, 100,  98],\n",
       "        [140, 114, 113],\n",
       "        [151, 130, 129],\n",
       "        [152, 136, 137]],\n",
       "\n",
       "       [[141, 113, 127],\n",
       "        [159, 131, 128],\n",
       "        [148, 122, 121],\n",
       "        [147, 126, 125],\n",
       "        [123, 107, 108]]], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This padding method only works on real value input.\n",
    "# real_image = sess.run(image)\n",
    "real_image = image\n",
    "crop_or_pad = tf.image.resize_image_with_crop_or_pad(\n",
    "    real_image, target_height=3, target_width=5)\n",
    "\n",
    "sess.run(crop_or_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLIPPING\n",
    "Flipping an image is exactly what it sounds like. Each pixel’s location is reversed horizontally or vertically. Technically speaking, flopping is the term used when flipping an image vertically. Terms aside, flipping images is useful with TensorFlow to give different perspectives of the same image for training. For example, a picture of an Australian Shepherd with crooked left ear could be flipped in order to allow matching of crooked right ears.\n",
    "\n",
    "TensorFlow has functions to flip images vertically, horizontally and choose randomly. The ability to randomly flip an image is a useful method to keep from overfitting a model to flipped versions of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[222, 232, 244],\n",
       "         [222, 232, 244]],\n",
       " \n",
       "        [[224, 234, 246],\n",
       "         [224, 234, 246]]], dtype=uint8)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_left_pixels = tf.slice(image, [0, 0, 0], [2, 2, 3])\n",
    "\n",
    "flip_horizon = tf.image.flip_left_right(top_left_pixels)\n",
    "flip_vertical = tf.image.flip_up_down(flip_horizon)\n",
    "\n",
    "sess.run([top_left_pixels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[224, 234, 246],\n",
       "        [224, 234, 246]],\n",
       "\n",
       "       [[222, 232, 244],\n",
       "        [222, 232, 244]]], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(flip_vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[222, 232, 244],\n",
       "        [222, 232, 244]],\n",
       "\n",
       "       [[224, 234, 246],\n",
       "        [224, 234, 246]]], dtype=uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(flip_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[224, 234, 246],\n",
       "        [224, 234, 246]],\n",
       "\n",
       "       [[222, 232, 244],\n",
       "        [222, 232, 244]]], dtype=uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_left_pixels = tf.slice(image, [0, 0, 0], [2, 2, 3])\n",
    "\n",
    "random_flip_horizon = tf.image.random_flip_left_right(top_left_pixels)\n",
    "random_flip_vertical = tf.image.random_flip_up_down(random_flip_horizon)\n",
    "\n",
    "sess.run(random_flip_vertical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SATURATION AND BALANCE\n",
    "Images which are found on the internet are often edited in advance. For instance, many of the images found in the Stanford Dogs dataset have too much saturation (lots of color). When an edited image is used for training, it may mislead a CNN model into finding patterns which are related to the edited image and not the content in the image.\n",
    "\n",
    "TensorFlow has useful functions which help in training on images by changing the saturation, hue, contrast and brightness. The functions allow for simple manipulation of these image attributes as well as randomly altering these attributes. The random altering is useful in training in for the same reason randomly flipping an image is useful. The random attribute changes help a CNN be able to accurately match a feature in images which have been edited or were taken under different lighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 254.19999695,    2.20000005,   15.19999981], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_red_pixel = tf.constant([254., 2., 15.])\n",
    "adjust_brightness = tf.image.adjust_brightness(example_red_pixel, 0.2)    #0.2 -> alpha\n",
    "\n",
    "sess.run(adjust_brightness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example brightens a single pixel, which is primarily red, with a delta of 0.2. Unfortunately, in the current version of TensorFlow 0.9, this method doesn’t work well with a tf.uint8 input. It’s best to avoid using this when possible and preprocess brightness changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[156, 164, 176],\n",
       "        [156, 164, 176],\n",
       "        [157, 164, 176]]], dtype=uint8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjust_contrast = tf.image.adjust_contrast(image, -.5)\n",
    "\n",
    "sess.run(tf.slice(adjust_contrast, [1, 0, 0], [1, 3, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example code adjusts the hue found in the image to make it more colorful. The adjustment accepts a delta parameter which controls the amount of hue to adjust in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[237, 241, 246],\n",
       "        [237, 241, 246],\n",
       "        [237, 241, 247]]], dtype=uint8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjust_saturation = tf.image.adjust_saturation(image, 0.4)\n",
    "\n",
    "sess.run(tf.slice(adjust_saturation, [1, 0, 0], [1, 3, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors\n",
    "CNNs are commonly trained using images with a single color. When an image has a single color it is said to use a grayscale colorspace meaning it uses a single channel of colors. For most computer vision related tasks, using grayscale is reasonable because the shape of an image can be seen without all the colors. The reduction in colors equates to a quicker to train image. Instead of a 3 component rank 1 tensor to describe each color found with RGB, a grayscale image requires a single component rank 1 tensor to describe the amount of gray found in the image.\n",
    "\n",
    "Although grayscale has benefits, it’s important to consider applications which require a distinction based on color. Color in images is challenging to work with in most computer vision because it isn’t easy to mathematically define the similarity of two RGB colors. In order to use colors in CNN training, it’s useful to convert the colorspace the image is natively in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAYSCALE\n",
    "Grayscale has a single component to it and has the same range of color as RGB left-bracket 0 comma 255 right-bracket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[230],\n",
       "        [230],\n",
       "        [229]]], dtype=uint8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray = tf.image.rgb_to_grayscale(image)\n",
    "\n",
    "sess.run(tf.slice(gray, [0, 0, 0], [1, 3, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gray_image = tf.image.rgb_to_grayscale(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grayscale_image = sess.run(gray_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 35, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grayscale_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(grayscale_image).ndim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(grayscale_image).shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSV\n",
    "Hue, saturation and value are what make up HSV colorspace. This space is represented with a 3 component rank 1 tensor similar to RGB. HSV is not similar to RGB in what it measures, it’s measuring attributes of an image which are closer to human perception of color than RGB. It is sometimes called HSB, where the B stands for brightness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.59090912,  0.09016392,  0.95686281],\n",
       "        [ 0.59090912,  0.09016392,  0.95686281],\n",
       "        [ 0.58974361,  0.10612243,  0.96078438]],\n",
       "\n",
       "       [[ 0.59090912,  0.08943088,  0.96470594],\n",
       "        [ 0.59090912,  0.08943088,  0.96470594],\n",
       "        [ 0.58974361,  0.10526314,  0.96862751]],\n",
       "\n",
       "       [[ 0.59090912,  0.08870967,  0.97254908],\n",
       "        [ 0.59090912,  0.08870967,  0.97254908],\n",
       "        [ 0.58974361,  0.10441766,  0.97647065]]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsv = tf.image.rgb_to_hsv(tf.image.convert_image_dtype(image, tf.float32))\n",
    "\n",
    "sess.run(tf.slice(hsv, [0, 0, 0], [3, 3, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB\n",
    "RGB is the colorspace which has been used in all the example code so far. It’s broken up into a 3 component rank 1 tensor which includes the amount of red left-bracket 0 comma 255 right-bracket, green left-bracket 0 comma 255 right-bracket and blue left-bracket 0 comma 255 right-bracket. Most images are already in RGB but TensorFlow has builtin functions in case the images are in another colorspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgb_hsv = tf.image.hsv_to_rgb(hsv)\n",
    "rgb_grayscale = tf.image.grayscale_to_rgb(gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASTING IMAGES\n",
    "In these examples, tf.to_float is often used in order to illustrate changing an image’s type to another format. For examples, this works OK but TensorFlow has a built in function to properly scale values as they change types. tf.image.convert_image_dtype(image, dtype, saturate=False) is a useful shortcut to change the type of an image from tf.uint8 to tf.float."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
